name: CI / PR / vllm-rbln with optimum

on:
  workflow_dispatch:
    inputs:
      ref:
        description: "ref to checkout"
        required: false
        type: string
      rebel_compiler_version:
        description: "Version of rebel-compiler to use (optional)"
        required: false
        type: string
      optimum_rbln_version:
        description: "Version of optimum-rbln to use (optional)"
        required: false
        type: string
  workflow_call:
    inputs:
      ref:
        description: "ref to checkout"
        required: false
        type: string
      rebel_compiler_version:
        description: "Version of rebel-compiler to use (optional)"
        required: false
        type: string
      optimum_rbln_version:
        description: "Version of optimum-rbln to use (optional)"
        required: false
        type: string

env:
  REBEL_PYPI_ENDPOINT: ${{ vars.REBEL_PYPI_INTERNAL_ENDPOINT }}
  REBEL_PYPI_USERNAME: ${{ secrets.REBEL_PYPI_USERNAME }}
  REBEL_PYPI_PASSWORD: ${{ secrets.REBEL_PYPI_PASSWORD }}
  REBEL_VLLM_PRE_COMPILED_DIR: ${{ secrets.VLLM_PRE_COMPILED_DIR }}

jobs:
  test_vllm_rbln:
    if: ${{ inputs.rebel_compiler_version && inputs.optimum_rbln_version }}
    runs-on: vllm-rbln-runner-c24m384a8
    env:
      NUM_INPUT_PROMPT: 1
      RBLN_VERBOSE: 6
      VLLM_LOGGING_LEVEL: DEBUG
    steps:
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Get latest rebel_compiler version
        if: ${{ !inputs.rebel_compiler_version }}
        id: get_latest_rebel_compiler
        run: |
          echo "LATEST_COMPILER_VER=${{ needs.get-compiler-version.outputs.compiler_version }}" >> $GITHUB_OUTPUT

      - name: Install rebel-compiler
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential
          export CXX=$(which g++)
          python3 -m pip uninstall rebel-compiler -y
          PYPI_URL=$(echo ${{ env.REBEL_PYPI_ENDPOINT }} | sed "s/\/\//\0${{ env.REBEL_PYPI_USERNAME }}:${{ env.REBEL_PYPI_PASSWORD }}@/")
          VERSION=${{ inputs.rebel_compiler_version || steps.get_latest_rebel_compiler.outputs.LATEST_COMPILER_VER }}
          python3 -m pip install --extra-index-url $PYPI_URL rebel-compiler==${VERSION}

      - name: Checkout current repository
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.pr_number && format('refs/pull/{0}/merge', inputs.pr_number) || inputs.ref || github.sha }}
          submodules: recursive
          fetch-depth: 0

      - name: Uninstall existing vllm-rbln
        run: |
          python3 -m pip uninstall -y vllm-rbln || true

      - name: Build vllm-rbln wheel
        run: |
          python -m pip install build
          python -m build --wheel

      - name: Install local vllm-rbln package and dependencies
        run: |
          pip install packaging setuptools wheel simphile pynvml huggingface_hub setuptools_scm fire
          VERSION=${{ inputs.optimum_rbln_version }}
          pip install --force-reinstall --no-cache-dir dist/vllm_rbln*.whl --constraint <(echo "optimum-rbln==$VERSION")
          echo "optimum-rbln $VERSION"

      - name: Run decoder-only test (eager attn) (V1)
        run: >
          python3 examples/optimum/run_decoder_only.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/llama2-7b_batch2
          --prompt_txt ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/prompts/copy_prompts.txt
          --golden_json ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/golden/golden_llama7b_result_copy_prompts.json

      - name: Run decoder-only test (Flash-attention mode) (V1)
        run: >
          python3 examples/optimum/run_decoder_only.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/llama3_2-3b-128k_kv16k_batch4
          --prompt_txt ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/prompts/copy_prompts.txt
          --golden_json ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/golden/golden_llama3_2_3b_instruct_128k_copy_prompts.json

      - name: Run gemma2 model (hybrid attention) (V1)
        run: >
          python3 examples/optimum/run_basic.py
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/gemma-2-2b-8k_batch4

      - name : Run Llava-next (Eager mode) (V1)
        run: >
           python3 examples/optimum/run_llava.py
           --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
           --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/llava-v1.6-mistral-7b-hf-32k-b4/
      
      - name : Run Llava-next (Eager mode) (V0)
        run: >
           VLLM_USE_V1=0 python3 examples/optimum/run_llava.py
           --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
           --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/llava-v1.6-mistral-7b-hf-32k-b4/

      - name : Run Llava-next (Flash-attention mode) (V1)
        run: >
          python3 examples/optimum/run_llava.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/llava-v1.6-mistral-7b-hf-32k-b4-kv16k
      
      - name : Run Llava-next (Flash-attention mode) (V0)
        run: >
          VLLM_USE_V1=0 python3 examples/optimum/run_llava.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/llava-v1.6-mistral-7b-hf-32k-b4-kv16k

      - name : Run Idefics3 (Eager mode) (V1)
        run: >
          python3 examples/optimum/run_idefics3.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/idefics3-8b-llama3-32k-b4

      - name : Run Idefics3 (Flash-attention mode) (V1)
        run: >
          python3 examples/optimum/run_idefics3.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/idefics3-8b-llama3-32k-b4-kv16k

      - name : Run Blip2 (V1)
        run: >
          python3 examples/optimum/run_blip2.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/blip2-opt-2.7b-2k-b4

      - name : Run Qwen2.5_VL (V1)
        run: >
          python3 examples/optimum/run_qwen_vl.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/qwen2_5-vl-7b-32k-b4-kv16k

      - name : Run PaliGemma (Eager mode) (V1)
        run: >
          python3 examples/optimum/run_paligemma.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/paligemma-3b-8k-b4

      - name : Run encoder-decoder (V1)
        run: >
          python3 examples/optimum/run_encoder_decoder.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/rbln_bart-small_batch2

      - name : Run text embedding model (V1)
        run: >
          python3 examples/optimum/run_encoder_only.py
          --num_input_prompt ${{ env.NUM_INPUT_PROMPT }}
          --model_id ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/bge-m3-1k-batch4
          --q_prompt_txt ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/prompts/q_prompts.txt
          --p_prompt_txt ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/prompts/p_prompts.txt
          --golden_json ${{ env.REBEL_VLLM_PRE_COMPILED_DIR }}/golden/golden_bge_m3_result_qp_prompts.json
